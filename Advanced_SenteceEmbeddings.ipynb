{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vNNm9doNKJ5"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install datasets evaluate\n",
        "!pip install --upgrade accelerate\n",
        "#!pip install transformers==4.28.0\n",
        "!pip install -U transformers\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install tasknet tasksource\n",
        "!pip install conllu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8HDqGXENPFX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from transformers import BertTokenizer, AutoTokenizer, AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
        "from transformers.utils import logging\n",
        "from tasknet import Adapter\n",
        "import tasksource\n",
        "from transformers import AutoModel, AutoModelForSequenceClassification, TextClassificationPipeline, AutoTokenizer\n",
        "import os\n",
        "from datasets import load_dataset, Dataset, load_from_disk\n",
        "import datasets\n",
        "from huggingface_hub import login\n",
        "from dataclasses import dataclass\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from typing import Optional, Union\n",
        "import torch\n",
        "import json\n",
        "import pandas as pd\n",
        "import random\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIMkUi00OkNJ"
      },
      "outputs": [],
      "source": [
        "# login to huggingface hub\n",
        "login(token=\"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\")\n",
        "\n",
        "# NOTE: this part is not needed if not running on collab\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# NOTE: navigate to the folder with dataset\n",
        "folder = '/content/drive/My Drive/CS4NLP'\n",
        "os.chdir(folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"CUDA is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA is not available, using CPU\")"
      ],
      "metadata": {
        "id": "ieyyHbmk7k4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKG4zryyOxJ0"
      },
      "outputs": [],
      "source": [
        "# install sentence transformers and get one model\n",
        "!pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "sentembb_model = SentenceTransformer('all-MiniLM-L6-v2',  device='cuda') # sentence embedding model https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5mu3K8tNkKc"
      },
      "outputs": [],
      "source": [
        "#from advanced_retrieval import sentence_embedding_cut\n",
        "from baseline_models import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp76XfeCPo4I"
      },
      "outputs": [],
      "source": [
        "ds_train = load_from_disk('datasets/quality/train')\n",
        "ds_dev = load_from_disk('datasets/quality/dev')\n",
        "\n",
        "print(f\"size of ds_train:{len(ds_train)}\")\n",
        "print(f\"size of ds_dev:{len(ds_dev)}\")\n",
        "# the labels start at 1 and not 0\n",
        "options = []\n",
        "print(ds_train[0])\n",
        "for row in ds_dev:\n",
        "  options.append(row[\"gold_label\"])\n",
        "print(np.unique(options))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLL9Y-6fAFVW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYH27tsQ_rh4"
      },
      "outputs": [],
      "source": [
        "# retrieval method\n",
        "\n",
        "def sentence_embedding_cut(article, tokenizer, query, MAX_TOKENS = 512, extra_length = 0, *args, **kwargs):\n",
        "    MAX_TOKENS = MAX_TOKENS - extra_length\n",
        "\n",
        "    sentences = article.split(\". \")\n",
        "    sentences = sentences[:-1] if not sentences[-1].strip() else sentences\n",
        "    sentences = [sentence if sentence.endswith(\".\") else sentence + \".\" for sentence in sentences]\n",
        "\n",
        "\n",
        "    query_embedding = sentembb_model.encode(query)\n",
        "\n",
        "    batch_size = 500\n",
        "    num_sentences = len(sentences)\n",
        "    num_batches = int(np.ceil(num_sentences / batch_size))\n",
        "\n",
        "    similarity_scores = []\n",
        "\n",
        "    # get all passage embeddings in batches\n",
        "    for i in range(num_batches):\n",
        "      start_index = i * batch_size\n",
        "      end_index = min((i + 1) * batch_size, num_sentences)\n",
        "\n",
        "      # Get the batch of sentences\n",
        "      batch_sentences = sentences[start_index:end_index]\n",
        "\n",
        "      # Encode the batch of sentences into embeddings\n",
        "      batch_embeddings = sentembb_model.encode(batch_sentences)\n",
        "\n",
        "      # Append the batch embeddings to the list\n",
        "      #passage_embedding.extend(batch_embeddings)\n",
        "\n",
        "      similarity = util.cos_sim(query_embedding, batch_embeddings).numpy()[0]\n",
        "      similarity_scores.extend(similarity)\n",
        "    #passage_embedding = sentembb_model.encode(sentences)\n",
        "\n",
        "    #similarity = util.cos_sim(query_embedding, passage_embedding).numpy()[0]\n",
        "    #print(\"Similarity:\", similarity_scores)\n",
        "\n",
        "    result = list(zip(range(0, len(sentences)), similarity_scores))\n",
        "\n",
        "    # sort them by similarity score\n",
        "    sentences_sortby_similarity = sorted(result, key=lambda x: x[1], reverse=True)\n",
        "    #print(sentences_sortby_similarity)\n",
        "\n",
        "\n",
        "    selected_sentences = []\n",
        "    total_tokens = 0\n",
        "\n",
        "    for (sentence_idx, similarity) in sentences_sortby_similarity:\n",
        "        tokens = tokenizer.tokenize(sentences[sentence_idx])\n",
        "        num_tokens = len(tokens)\n",
        "        if total_tokens == MAX_TOKENS:\n",
        "            break\n",
        "        elif (total_tokens + num_tokens) <= MAX_TOKENS:\n",
        "            selected_sentences.append(sentence_idx)\n",
        "            total_tokens += num_tokens\n",
        "        else:\n",
        "            break\n",
        "\n",
        "\n",
        "    # use the senteces in the original order\n",
        "    selected_sentences.sort()\n",
        "    selected_sentences = [sentences[i] for i in selected_sentences]\n",
        "\n",
        "    return \" \".join(selected_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xVKdsivgXgy"
      },
      "outputs": [],
      "source": [
        "model = RobertaLarge()\n",
        "max_length = model.get_max_seq_length()\n",
        "tokenizer = model.get_tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_length)"
      ],
      "metadata": {
        "id": "ap0OT9d_Shgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFruVwChy3Am"
      },
      "outputs": [],
      "source": [
        "# create csv file, with cut context by sentence embeddings\n",
        "ds_sentembb_train_path = 'datasets/sentembb_roberta/train'\n",
        "ds_sentembb_dev_path = 'datasets/sentembb_roberta/dev'\n",
        "\n",
        "if not os.path.exists(ds_sentembb_dev_path):\n",
        "  ds_sentembb_dict = {'cut_article': [], 'question': [], 'options' : [], 'label': []}\n",
        "  ctr = 0\n",
        "  total_len = len(ds_train)\n",
        "  for item in ds_train:\n",
        "    # article\n",
        "    article = item[\"article\"]\n",
        "    question = item[\"question\"]\n",
        "    options = item[\"options\"]\n",
        "    label = item[\"gold_label\"] - 1 # labels start at 1\n",
        "\n",
        "    extra_length = model.get_extra_input_length(question=question, options=options)\n",
        "\n",
        "    cut_article = sentence_embedding_cut(article=article, tokenizer=tokenizer, MAX_TOKENS=max_length, query=question, extra_length = extra_length)\n",
        "\n",
        "    #cut_item = {'cut_article': cut_article, 'question': question, 'options' : options, 'label': label}\n",
        "    ds_sentembb_dict['cut_article'].append(cut_article)\n",
        "    ds_sentembb_dict['question'].append(question)\n",
        "    ds_sentembb_dict['options'].append(options)\n",
        "    ds_sentembb_dict['label'].append(label)\n",
        "    ctr+=1\n",
        "    if (ctr) % 100 == 0:\n",
        "      print(f\"{ctr}/{total_len}\")\n",
        "\n",
        "\n",
        "  # save new dataset\n",
        "  new_dataset = Dataset.from_dict(ds_sentembb_dict)\n",
        "  os.makedirs(ds_sentembb_dev_path, exist_ok=True)\n",
        "\n",
        "  new_dataset.save_to_disk(ds_sentembb_dev_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NYU873sB-Y_"
      },
      "outputs": [],
      "source": [
        "#item = ds_dev[0]\n",
        "'''\n",
        "item = {'article' : \"Thomas went to the restroom. Abby was in the kitchen. Melinda set the table.\", 'question' : 'Who made the burger?', 'options': ['Melinda', 'Thomas', 'Abby', 'Fritz'], 'gold_label': 3}\n",
        "\n",
        "ds_sentembb_train_path = 'datasets/sentembb_longf/train'\n",
        "ds_sentembb_dev_path = 'datasets/sentembb_longf/dev'\n",
        "ds_sentembb_train_dict = {'cut_article': [], 'question': [], 'options' : [], 'label': []}\n",
        "\n",
        "if not os.path.exists(ds_sentembb_dev_path):\n",
        "  # article\n",
        "  article = item[\"article\"]\n",
        "  question = item[\"question\"]\n",
        "  options = item[\"options\"]\n",
        "  label = item[\"gold_label\"] - 1 # labels start at 1\n",
        "\n",
        "  extra_length = model.get_extra_input_length(question=question, options=options)\n",
        "\n",
        "  cut_article = sentence_embedding_cut(article=article, tokenizer=tokenizer, MAX_TOKENS=max_length, query=question, extra_length = extra_length)\n",
        "\n",
        "  #cut_item = {'cut_article': cut_article, 'question': question, 'options' : options, 'label': label}\n",
        "  ds_sentembb_train_dict['cut_article'].append(cut_article)\n",
        "  ds_sentembb_train_dict['question'].append(question)\n",
        "  ds_sentembb_train_dict['options'].append(options)\n",
        "  ds_sentembb_train_dict['label'].append(label)\n",
        "\n",
        "  #ds_sentembb_train.append(cut_item)\n",
        "\n",
        "  #print(type(ds_sentembb_train_dict))\n",
        "\n",
        "  new_dataset = Dataset.from_dict(ds_sentembb_train_dict)\n",
        "  os.makedirs(ds_sentembb_train_path, exist_ok=True)\n",
        "\n",
        "  new_dataset.save_to_disk(ds_sentembb_dev_path)\n",
        "else:\n",
        "  print('folder exists already')\n",
        "  '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUOPGL5kHAni"
      },
      "outputs": [],
      "source": [
        "#print(ds_dev[0])\n",
        "#print(len(ds_dev[0]['article']))\n",
        "\n",
        "test = load_from_disk(ds_sentembb_dev_path)\n",
        "print(test[0])\n",
        "print(len(test[0]['cut_article']))\n",
        "print(test[0]['question'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: run from here\n",
        "# init model\n",
        "model = Longformer() # change to Roberta/Transformer\n",
        "device = model.device\n",
        "print(device)"
      ],
      "metadata": {
        "id": "T3E0iZA6dI-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "799SfjDOHVpv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYy1U3XiNwnB"
      },
      "outputs": [],
      "source": [
        "model_name = 'Longformer' # output\n",
        "retrieval = 'Sentence_Embeddings' #for output\n",
        "ds_sentembb_dev_path = 'datasets/sentembb_longformer/dev'\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "df = pd.DataFrame(columns=[\"preprocessor\", \"model\", \"accuracy\"])\n",
        "\n",
        "ds_sentembb_dev = load_from_disk(ds_sentembb_dev_path)\n",
        "\n",
        "references = []\n",
        "predictions = []\n",
        "ctr = 0\n",
        "#max_length = model.get_max_seq_length()\n",
        "#tokenizer = model.get_tokenizer()\n",
        "\n",
        "for item in ds_sentembb_dev:\n",
        "  # parse dataset\n",
        "  article = item[\"cut_article\"]\n",
        "  question = item[\"question\"]\n",
        "  options = item[\"options\"]\n",
        "  label = item[\"label\"]\n",
        "\n",
        "  prediction = model.predict(context=article, question=question, options=options)\n",
        "\n",
        "  #prediction = 1\n",
        "  # metrics\n",
        "  predictions.append(prediction)\n",
        "  references.append(label)\n",
        "\n",
        "  ctr+=1\n",
        "  if (ctr) % 100 == 0:\n",
        "    print(f\"{ctr}/{len(ds_sentembb_dev)}\")\n",
        "\n",
        "\n",
        "# save performance of one configuration\n",
        "metric = accuracy.compute(references=references, predictions=predictions)\n",
        "row = np.array((retrieval, model_name, metric), ndmin=2)\n",
        "df = pd.concat([df, pd.DataFrame(row, columns=[\"preprocessor\", \"model\", \"accuracy\"])], axis = 0)\n",
        "pred_df = pd.DataFrame({\"Prediction\": predictions, \"Label\": references})\n",
        "\n",
        "\n",
        "df.to_csv(retrieval+\"_\"+model_name+\".csv\", index=False)\n",
        "pred_df.to_csv(retrieval+\"_\"+model_name+\"_predictions_roberta.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''import torch\n",
        "\n",
        "# Move the model to the CUDA device\n",
        "model.model.to('cuda:0')\n",
        "print(model.device)\n",
        "# Assuming `context`, `question`, and `options` are non-tensor inputs\n",
        "context = \"William is hungry.\"\n",
        "question = \"Who ate the potato?\"\n",
        "options = [\"William\", \"James\", \"Feb\", \"March\"]\n",
        "\n",
        "\n",
        "# Perform prediction0\n",
        "predictions = model.predict(context=context, question=question, options=options)\n",
        "print(predictions)\n",
        "#inputs = model.prepare_answering_input(question=question, options=options, context=context).to(model.device)\n",
        "#outputs = model.model(**inputs)\n",
        "#print(outputs)'''"
      ],
      "metadata": {
        "id": "awXwmGFggFrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8gNeU7DDHCLe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}